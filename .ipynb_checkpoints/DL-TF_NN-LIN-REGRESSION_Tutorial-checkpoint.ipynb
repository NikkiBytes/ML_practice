{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49f0e1d-c451-4d87-b365-c4b4b1397469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72024af-76bc-4d83-a18c-9ba6a20100cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba267c98-9715-4059-9535-9827b441935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6eca85-d112-452e-8021-697ed9b435b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just some basic preprocessing\n",
    "cars_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ebc80-c8e4-46fa-8292-61da72433e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dummy variables for the categorical features\n",
    "cars_data = pd.get_dummies(cars_data)\n",
    "cars_data = cars_data.astype('float32') # we will need to convert the dataset to float in order to be able to convert it into tensors later.\n",
    "cars_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753126a9-76a8-42d7-b98a-9146402bef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring column names\n",
    "cars_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d1e97-3456-4d4c-b13d-1cc9183cfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the features and labels and finally splitting the test and train data.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = cars_data[['year','mileage', 'model_SE', 'model_SEL', 'model_SES',\n",
    "       'color_Black', 'color_Blue', 'color_Gold', 'color_Gray', 'color_Green',\n",
    "       'color_Red', 'color_Silver', 'color_White', 'color_Yellow',\n",
    "       'transmission_AUTO', 'transmission_MANUAL']]\n",
    "Y = cars_data['price']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f25ebd9-7166-47f8-b9ca-c68138b97cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us scale the data as features are on different scales which might be a problem while modelling\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "# MinMaxScalar has been used here. You can go ahead and use the other scalars available and chcek the effect on the results.\n",
    "#fitting the transform on test and train separately\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9fe8f-89eb-4aa7-8924-bc7397121590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us now convert the data elements into tensors as we need tensors to be fed into different tensorflow based operations\n",
    "#X-train and X_test were converted to numpy arrays while transformations while the other two need to be transformed into numpy arrays.\n",
    "X_train=tf.convert_to_tensor(X_train)\n",
    "y_train=tf.convert_to_tensor(y_train.values)\n",
    "X_test=tf.convert_to_tensor(X_test)\n",
    "y_test=tf.convert_to_tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb4de4-bb70-4ba1-a7a0-d846385f374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d771ae-a0e9-41c0-b195-92f78575c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Let us initialize the weights and bias variables. \n",
    "weights = tf.Variable(tf.zeros(shape=(input_dim, output_dim), dtype= tf.float32))\n",
    "bias = tf.Variable(tf.ones(shape=(output_dim,), dtype= tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc864b8-f0b2-4bba-bfc1-b5b9afbb9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f2a63-6b5a-4ea5-a909-565f7eed425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9edd323-a6ec-4beb-b6e7-914ec4f6bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features):\n",
    "    return tf.matmul(features, weights) + bias # note that the matmul is matrix multiplication and is needed for calculating predictions\n",
    "\n",
    "def compute_loss(y_true, predictions):\n",
    "    return tf.reduce_mean(tf.square(y_true - predictions)) # mean square error\n",
    "\n",
    "# Let us now define a function to train the model. We will call the other functions in function definition.\n",
    "def train(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = predict(x)\n",
    "        loss = compute_loss(y, predictions)\n",
    "        dloss_dw, dloss_db = tape.gradient(loss, [weights, bias]) #note that we can pass lists as well here.\n",
    "    weights.assign_sub(learning_rate * dloss_dw)\n",
    "    bias.assign_sub(learning_rate * dloss_db)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123148e6-d47a-4e12-8cf1-5014f070f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    loss = train(X_train, y_train)\n",
    "    print('Epoch %d: Loss = %.4f' % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622044a2-0d0a-4ef6-97c1-8257a78fbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Weights after 50 epochs:')\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff10e1-8797-45d9-a17d-9ee8efe67bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Bias after 50 epochs:')\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1bbf9-4420-4043-adf6-686484c04687",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = tf.matmul(X_test, weights) + bias\n",
    "print(compute_loss(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625d719f-1e84-47d3-85d2-3a9128d6c401",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
