{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection: choosing estimators and their parameters\n",
    "  \n",
    "Reference: https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html \n",
    "\n",
    "  \n",
    "    \n",
    "As seen, every estimator exposes a ```score``` method that can judge the quality of the fit(or the prediction) on new data. **Big is better.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm \n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "svc = svm.SVC(C=1, kernel='linear')\n",
    "svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better measure of prediction accuracy(which we can use as a proxy for goodness of fit of the model), we can successively split the data in *folds* that we use for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9348914858096828, 0.9565943238731218, 0.9398998330550918]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_folds = np.array_split(X_digits, 3)\n",
    "y_folds = np.array_split(y_digits, 3)\n",
    "scores = list()\n",
    "for k in range(3):\n",
    "# We use 'list' to copy, in order to 'pop' later on\n",
    "    X_train = list(X_folds)\n",
    "    X_test = X_train.pop(k)\n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_train = list(y_folds)\n",
    "    y_test = y_train.pop(k)\n",
    "    y_train = np.concatenate(y_train)\n",
    "    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(scores)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a **kfold** cross valdidation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation generators  \n",
    "\n",
    "\n",
    "Scikit-learn has a collection of class which can be used to generate lists of train/test indices for popular cross-validation strategies.  \n",
    "\n",
    "  \n",
    "They expose a ```split``` method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen CV strategy.  \n",
    "  \n",
    "This example shows an example usage of the split method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2 3 4 5 6 7 8 9] | test: [0 1]\n",
      "Train: [0 1 4 5 6 7 8 9] | test: [2 3]\n",
      "Train: [0 1 2 3 6 7 8 9] | test: [4 5]\n",
      "Train: [0 1 2 3 4 5 8 9] | test: [6 7]\n",
      "Train: [0 1 2 3 4 5 6 7] | test: [8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "X = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"c\", \"c\", \"c\"]\n",
    "k_fold = KFold(n_splits=5)\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "      print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CV can be performed easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9638888888888889,\n",
       " 0.9222222222222223,\n",
       " 0.9637883008356546,\n",
       " 0.9637883008356546,\n",
       " 0.9303621169916435]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])\n",
    "  for train, test in k_fold.split(X_digits)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CV score can be directly calculated using the cross_val_score helper. Given an estimator, the CV object and the input dataset, the ```cross_val_score``` splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.  \n",
    "  \n",
    "By default the ```score``` method is used to compute the individual scores.   \n",
    "\n",
    "Refer the [metrics module](https://scikit-learn.org/stable/modules/metrics.html#metrics) to learn more on the available scoring methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96388889, 0.92222222, 0.9637883 , 0.9637883 , 0.93036212])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_jobs=-1 means that the computation will be dispatched on all the CPUs of the computer.  \n",
    "  \n",
    "Alternatively, the scoring argument can be provided to specify an alternative scoring method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96578289, 0.92708922, 0.96681476, 0.96362897, 0.93192644])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(svc, X_digits, y_digits, cv=k_fold,\n",
    "                 scoring='precision_macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation generators  \n",
    "  \n",
    "**KFold (n_splits, shuffle, random_state)** - splits it into K folds, trains on K-1 and then tests on the left-out.  \n",
    "**StratifiedKFold (n_splits, shuffle, random_state)** - Same as K-Fold but preserves the class distrubution within each fold.  \n",
    "**GroupKFold (n_splits)** - Ensures that the same group is not in both testing and training sets.  \n",
    "**ShuffleSplit (n_splits, test_size, train_size, random_state)** - Generates train/test indices based on random permutation.  \n",
    "**StratifiedShuffleSplit** - Same as shuffle split but preserves the class distribution within each iteration.  \n",
    "**GroupShuffleSplit** - Ensures that the same group is no in both testing and training sets.  \n",
    "**LeaveOneGroupOut()** - Takes a group array to group observations.  \n",
    "**LeavePGroupsOut (n_groups)** - Leave P groups out.  \n",
    "**LeaveOneOut()** - Leave one observation out.  \n",
    "**LeavePOut (p)** - Leave P observations out.  \n",
    "**PredefinedSplit** - Generates train/test indices based on predefined splits.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-search and cross-validated estimators  \n",
    "  \n",
    "#### Grid-search  \n",
    "scikit-learn provodes an objevt that, given data, computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score. This objects takes an estimator during the construction and exposes and estimator API:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'C': array([1.00000e-06, 3.59381e-06, 1.29155e-05, 4.64159e-05, 1.66810e-04,\n",
       "       5.99484e-04, 2.15443e-03, 7.74264e-03, 2.78256e-02, 1.00000e-01])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),\n",
    "                    n_jobs=-1)\n",
    "clf.fit(X_digits[:1000], y_digits[:1000])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007742636826811277"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9435382685069009"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction performance on test set is not as good as on train set\n",
    "clf.score(X_digits[1000:], y_digits[1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the ```GridSearchCV``` uses a 3-fold cross-validation. However, if it detects that a classifier is passed, rather than a regressor, it uses a stratified 3-fold. The default will change to a 5-fold cross-validaton in version 0.22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93853821, 0.96327212, 0.94463087])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two cross-validation loops are performed in parallel: one by the ```GridSearchCV``` estimator to set gamma and the other one by cross_val_score to measure the prediction performance of the estimator. The resulting scores are unbiased estimates of the prediction score on new data.  \n",
    "  \n",
    "  Warning You cannot nest objects with parallel computing (n_jobs different than 1).  \n",
    "  \n",
    "  \n",
    "#### Cross-validated estimators \n",
    "Cross-validation to set a parameter can be done more efficiently on an algorithm-by-algorithm basis. This is why for certain estimators, scikit-learn exposes Cross-Validation:evaluating estimator performance estimators that set their parameter automatically by cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=3, eps=0.001, fit_intercept=True,\n",
       "    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,\n",
       "    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "lasso = linear_model.LassoCV(cv=3)\n",
    "diabetes = datasets.load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "lasso.fit(X_diabetes, y_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012291895087486173"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.alpha_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
